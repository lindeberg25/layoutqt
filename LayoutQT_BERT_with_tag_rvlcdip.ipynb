{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2358,"status":"ok","timestamp":1666562032796,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"GLxgv9OSFqYi","outputId":"da64825f-45da-49d7-b648-f90fb768bc9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/LayoutQT')\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84QeWKhKKG_S","executionInfo":{"status":"ok","timestamp":1666562036275,"user_tz":180,"elapsed":3492,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"2be4712e-c3ff-4404-ed8f-9beae079aea9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXfagt2R4ysX"},"outputs":[],"source":["import torch\n","\n","# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqfZpHuo4oPd"},"outputs":[],"source":["def process_data_for_bert(data):\n","\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    X_tokenized = tokenizer(list(data['texttags']), padding=True, truncation=True, max_length=512)\n","    y = list(data['label'])\n","    \n","    dataset = Dataset(X_tokenized, y)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVekXFaKy38u"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoModelForMaskedLM\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","\n","def runBert(df_train_dataset, df_val_dataset, df_test_dataset):\n","    \n","    batch_size=16\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=16)\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=False )\n","     \n","    train_dataset = process_data_for_bert(df_train_dataset)\n","    val_dataset = process_data_for_bert(df_val_dataset)\n","    test_dataset = process_data_for_bert(df_test_dataset)\n","\n","    # ----- 2. Fine-tune pretrained model -----#\n","    # Define Trainer parameters\n","    def compute_metrics(p):\n","        \n","        pred, labels = p\n","        pred = np.argmax(pred, axis=1)\n","\n","        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","        recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n","        precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n","        f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n","\n","        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","    # Define Trainer\n","    args = TrainingArguments(\n","        \"./model/layoutqt_rvlcdip\",\n","        evaluation_strategy = \"epoch\",\n","        save_strategy = \"epoch\",\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        num_train_epochs=5,\n","        weight_decay=0.01,\n","        load_best_model_at_end=True,\n","        metric_for_best_model='f1'\n","      # optim=\"adamw_torch\"\n","    )\n","\n","    def model_init():\n","        return model\n","\n","    trainer = Trainer(\n","        model_init=model_init,\n","        args=args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    # New code - wrap collator in a dictionary\n","    #data_collator = trainer.data_collator\n","    #trainer.data_collator = lambda data: dict(data_collator(data))\n","    # End new code\n","\n","    trainer.train()\n","\n","    # Make prediction\n","    raw_pred, _, _ = trainer.predict(test_dataset)\n","\n","    # Preprocess raw predictions\n","    y_pred = np.argmax(raw_pred, axis=1)\n","\n","    return y_pred\n","    #return roc_auc_score(df_test_dataset['class'], y_pred), f1_score(df_test_dataset['class'], y_pred, average='macro')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUTqnUToLjAH","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"24b48069-0152-46e0-c302-789ed662cb66","executionInfo":{"status":"ok","timestamp":1666629204892,"user_tz":180,"elapsed":2967884,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 124986\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 39060\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='38192' max='39060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [38192/39060 17:26:37 < 23:47, 0.61 it/s, Epoch 4.89/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685900</td>\n","      <td>0.662924</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.533500</td>\n","      <td>0.615903</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.410300</td>\n","      <td>0.639670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.299300</td>\n","      <td>0.702951</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 2060\n","  Batch size = 16\n","Saving model checkpoint to ./model/layoutqt_rvlcdip/checkpoint-7812\n","Configuration saved in ./model/layoutqt_rvlcdip/checkpoint-7812/config.json\n","Model weights saved in ./model/layoutqt_rvlcdip/checkpoint-7812/pytorch_model.bin\n","tokenizer config file saved in ./model/layoutqt_rvlcdip/checkpoint-7812/tokenizer_config.json\n","Special tokens file saved in ./model/layoutqt_rvlcdip/checkpoint-7812/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 2060\n","  Batch size = 16\n","Saving model checkpoint to ./model/layoutqt_rvlcdip/checkpoint-15624\n","Configuration saved in ./model/layoutqt_rvlcdip/checkpoint-15624/config.json\n","Model weights saved in ./model/layoutqt_rvlcdip/checkpoint-15624/pytorch_model.bin\n","tokenizer config file saved in ./model/layoutqt_rvlcdip/checkpoint-15624/tokenizer_config.json\n","Special tokens file saved in ./model/layoutqt_rvlcdip/checkpoint-15624/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 2060\n","  Batch size = 16\n","Saving model checkpoint to ./model/layoutqt_rvlcdip/checkpoint-23436\n","Configuration saved in ./model/layoutqt_rvlcdip/checkpoint-23436/config.json\n","Model weights saved in ./model/layoutqt_rvlcdip/checkpoint-23436/pytorch_model.bin\n","tokenizer config file saved in ./model/layoutqt_rvlcdip/checkpoint-23436/tokenizer_config.json\n","Special tokens file saved in ./model/layoutqt_rvlcdip/checkpoint-23436/special_tokens_map.json\n","***** Running Evaluation *****\n","  Num examples = 2060\n","  Batch size = 16\n","Saving model checkpoint to ./model/layoutqt_rvlcdip/checkpoint-31248\n","Configuration saved in ./model/layoutqt_rvlcdip/checkpoint-31248/config.json\n","Model weights saved in ./model/layoutqt_rvlcdip/checkpoint-31248/pytorch_model.bin\n","tokenizer config file saved in ./model/layoutqt_rvlcdip/checkpoint-31248/tokenizer_config.json\n","Special tokens file saved in ./model/layoutqt_rvlcdip/checkpoint-31248/special_tokens_map.json\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='39060' max='39060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [39060/39060 17:51:37, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685900</td>\n","      <td>0.662924</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","      <td>0.802913</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.533500</td>\n","      <td>0.615903</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","      <td>0.823301</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.410300</td>\n","      <td>0.639670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","      <td>0.827670</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.299300</td>\n","      <td>0.702951</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","      <td>0.828641</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.244200</td>\n","      <td>0.744823</td>\n","      <td>0.830097</td>\n","      <td>0.830097</td>\n","      <td>0.830097</td>\n","      <td>0.830097</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 2060\n","  Batch size = 16\n","Saving model checkpoint to ./model/layoutqt_rvlcdip/checkpoint-39060\n","Configuration saved in ./model/layoutqt_rvlcdip/checkpoint-39060/config.json\n","Model weights saved in ./model/layoutqt_rvlcdip/checkpoint-39060/pytorch_model.bin\n","tokenizer config file saved in ./model/layoutqt_rvlcdip/checkpoint-39060/tokenizer_config.json\n","Special tokens file saved in ./model/layoutqt_rvlcdip/checkpoint-39060/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./model/layoutqt_rvlcdip/checkpoint-39060 (score: 0.8300970873786409).\n","***** Running Prediction *****\n","  Num examples = 39999\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}],"source":["import numpy as np\n","import pandas as pd\n","\n","             \n","df_train = pd.read_csv(\"./input/rvlcdip/train_tags.csv\")\n"," \n","df_val = pd.read_csv(\"./input/rvlcdip/val_tags.csv\")\n","\n","df_test = pd.read_csv(\"./input/rvlcdip/test_tags.csv\")\n","\n","df_train['texttags'] = df_train['texttags'].apply(lambda x: str(x))\n","df_test['texttags'] = df_test['texttags'].apply(lambda x: str(x))\n","\n","y_pred = runBert(df_train, df_val, df_test)"]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","print(\"Accuracy:\", accuracy_score(df_test['label'], y_pred))\n","print(\"F1:\", f1_score(df_test['label'], y_pred, average='micro'))\n"],"metadata":{"id":"T0Oo3lP83iyn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666629204907,"user_tz":180,"elapsed":13,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"4000b8d2-6ad1-4cbe-bdff-93672dc0f03b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8423710592764819\n","F1: 0.8423710592764819\n"]}]},{"cell_type":"code","source":["print(classification_report(df_test['label'], y_pred))"],"metadata":{"id":"r7OmwUESA9Ab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666629204908,"user_tz":180,"elapsed":6,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"}},"outputId":"1532375a-745b-4783-e29f-30c19db10cdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.85      0.86      2464\n","           1       0.78      0.78      0.78      2506\n","           2       0.97      0.96      0.96      2516\n","           3       0.81      0.79      0.80      2531\n","           4       0.71      0.72      0.71      2515\n","           5       0.80      0.80      0.80      2498\n","           6       0.91      0.87      0.89      2572\n","           7       0.92      0.90      0.91      2472\n","           8       0.67      0.82      0.74      2527\n","           9       0.84      0.81      0.83      2463\n","          10       0.82      0.79      0.81      2505\n","          11       0.86      0.84      0.85      2477\n","          12       0.79      0.81      0.80      2489\n","          13       0.88      0.86      0.87      2435\n","          14       0.99      0.98      0.98      2537\n","          15       0.90      0.89      0.90      2492\n","\n","    accuracy                           0.84     39999\n","   macro avg       0.85      0.84      0.84     39999\n","weighted avg       0.85      0.84      0.84     39999\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}